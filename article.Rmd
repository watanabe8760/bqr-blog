---
title: "Google BigQuery with R"
output: html_document
---

`dplyr` is one of the most efficient packages to manipulate table data in the R language environment. It provides a consistent set of verbs that help us solve the most common data manipulation challenges. In this post I’d like to share how we can utilize the grammar of `dplyr` to manipulate data stored in Google BigQuery without writing any SQL.

## Target audience
I expect readers already know the basics of `dplyr` and BigQuery thus will skip the preliminary. When I started to use `dplyr` with BigQuery, I was already familiar with `dplyr` but still somehow struggled to get used to it because I couldn't find any comprehensive resource online. So my motivation here is to provide a practical guideline.

## Memo, these can be inserted somewhere
- The flow of the coding matches the flow of thinking.
- The expressions are very concise.
- After writing this post, I noticed that the most of things written here are also applicable to other databases.


## Data
The sample data used in this tutorial is Iowa Liquor Product Sales dataset. The following csv files are downloaded from [here](https://data.world/classrooms/guide-to-data-analysis-with-sql-and-datadotworld).

- products.csv
- stores.csv
- stores_convenience.csv
- sales.csv

Before uploading csv files to BigQuery, I had to make some modifications.

- Remove newline characters (\n).
- Double-quote string columns.

Then datasets and tables are created from [BigQuery Web UI](https://console.cloud.google.com/bigquery). sales.csv is a little less than 500 MB so that I had to upload the file to GCS to create the table.

The datasets and tables in BigQuery Web UI look like this.
[BigQuery screen shot](bq_screen_shot.png)


## Packanges
```{r}
library(bigrquery)
library(dplyr)
```

TODO: confusion between bigrquery and dbplyr


## Connection
You can create a connection to BigQuery using `bigrquery::dbConnect`. There are two ways to establish the connection depending on how you specify the dataset name. `dplyr::tbl` gives you a reference to the table specified.

```{r}
# connection pattern 1
bq_con <-
  dbConnect(drv = bigquery(),
            project = "iowa-liquor-product-sales")

bq_con %>% tbl("transaction.sales")
```

```{r}
# connection pattern 2
bq_con <-
  dbConnect(drv = bigquery(),
            project = "iowa-liquor-product-sales",
            dataset = "transaction")

bq_con %>% tbl("sales")
```

At the first glance, the pattern 2 looks more concise because you don’t have to repeat the dataset name when you access the same table several times. But there is a catch. If you specify the dataset name in `dbConnect`, you cannot use the same connection to access tables in different datasets. Generally speaking, accessing tables in different datasets is very common (e.g. Join two tables in different datasets) so that the pattern 1 is more flexible and convenient.

I will use a connection created by the pattern 1 all through this post.

## Write a query
Once you have the connection to BigQuery, you can read and manipulate all datasets and tables in the project as if they are `data.frame` by using `dplyr` verbs.

```{r}
# Summarize the sales record of May 2014 by category.
sales_in_may_query <-
  bq_con %>%
  tbl("transaction.sales") %>%
  select(date, btl_price, state_btl_cost, category_name, bottle_qty, total) %>%
  filter(date >= "2014-05-01",
         date <= "2014-05-31") %>%
  mutate(gross_profit = btl_price - state_btl_cost) %>%
  group_by(category_name) %>%
  summarise(n_bottles_sold = sum(bottle_qty, na.rm = TRUE),
            revenue = sum(total, na.rm = TRUE),
            gross_profit = sum(gross_profit, na.rm = TRUE)) %>%
  arrange(desc(gross_profit))
```

When this code is executed, what's stored in `sales_in_may` is just a BigQuery object, not the actual data retrieved by the query. To download the data to the local R environment, `collect()` has to be called explicitly. Then the result of query becomes a local `data.frame`. All the verbs before `collect()` are executed in the BigQuery side as it is translated into SQL automatically.

```{r}
sales_in_may_df <-
  sales_in_may_query %>%
  collect()

sales_in_may_df
```

If you intend to download the data from the beginning, you can just add `collect()` at the end of piping. This way the query immediately materializes a local `data.frame` without keeping the BigQuery object in the local environment.

```{r}
sales_in_may_df <-
  bq_con %>%
  tbl("transaction.sales") %>%
  ... %>%
  collect()
```

Note, before downloading data from BigQuery to the local environment it is better to process data in the BigQuery side as much as possible. Two reasons.

1. BigQuery is very fast to perform any data manipulation and the processing itself doesn't cost you. (It charges you by the amount of data scanned by the query.)
2. If you download a big chunk of data, it might take a long time. If it's too big, it can fail after you wait for a while.

If you want to check what SQL is actually executed, you can see it by `dbplyr::sql_render()`. `dbplyr` package is the database backend for `dplyr` which translates all the verbs of `dplyr` into SQL. This package is implicitly used behind `bigrquery`.

```{r}
sales_in_may_query %>%
  dbplyr::sql_render()
```

The SQL is nested and not clean, but you can get the gist of it. 


## Create dataset and table

It's as simple as follows.

```{r}
bq_dataset(project = "iowa-liquor-product-sales",
           dataset = "summary") %>%
  bq_dataset_create()
```

```{r}
bq_table(project = "iowa-liquor-product-sales",
         dataset = "summary",
         table   = "sales_in_may") %>%
  bq_table_create(sales_in_may_df)
```

For table creation, you need to specify the field definition. The easiest way to do this is to pass a `data.frame` that you are planning to upload. `bigrquery::bq_table_create()` automatically converts it to the field definition. If you don't have any target `data.frame`, you can manually specify by `bq_field()` and `bq_fields()`.

Note, you don't need the connection when you create dataset and table. All you have to do is to specify the names by characters. 


## Upload data
When you have a local `data.frame` that you want to store in BigQuery, you can do it with `bigrquery::bq_table_upload()`. 

```{r}
bq_table(project = "iowa-liquor-product-sales",
         dataset = "summary",
         table   = "sales_in_may") %>%
  bq_table_upload(sales_in_may_df)
```

Table creation and uploading a local data.frame to the table can be done in one line.

```{r}
bq_table(project = "iowa-liquor-product-sales",
         dataset = "summary",
         table   = "sales_in_may") %>%
  bq_table_create(sales_in_may_df) %>%
  bq_table_upload(sales_in_may_df)
```



TODO: Complete in GCP



## Use SQL standard functions


## When SQL standard function do not follow R syntax


## Limitation
- Per-row processing (furrr, map operation)
- Pivot (pivot_wider, pivot_longer)
- Download & upload capacity



